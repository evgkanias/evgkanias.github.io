<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Evripidis Gkanias</title>
    <link>https://evgkanias.github.io/</link>
      <atom:link href="https://evgkanias.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Evripidis Gkanias</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-gb</language><copyright>© 2024 Evripidis Gkanias</copyright><lastBuildDate>Fri, 09 Aug 2024 08:00:00 +0000</lastBuildDate>
    <image>
      <url>https://evgkanias.github.io/media/icon_hu22afa8837ab305f0f3e5e2b9e7b7c4db_6290075_512x512_fill_lanczos_center_3.png</url>
      <title>Evripidis Gkanias</title>
      <link>https://evgkanias.github.io/</link>
    </image>
    
    <item>
      <title>Spatiotemporal computations in the insect celestial compass</title>
      <link>https://evgkanias.github.io/publication/spatiotemporal-computations-in-the-insect-celestial-compass/</link>
      <pubDate>Fri, 09 Aug 2024 08:00:00 +0000</pubDate>
      <guid>https://evgkanias.github.io/publication/spatiotemporal-computations-in-the-insect-celestial-compass/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Time compensation in the celestial compass of insects</title>
      <link>https://evgkanias.github.io/talk/time-compensation-in-the-celestial-compass-of-insects/</link>
      <pubDate>Wed, 31 Jul 2024 09:00:00 +0000</pubDate>
      <guid>https://evgkanias.github.io/talk/time-compensation-in-the-celestial-compass-of-insects/</guid>
      <description>&lt;object data=&#34;icn2024.pdf&#34; type=&#34;application/pdf&#34; width=&#34;700px&#34; height=&#34;1020px&#34;&gt;
&lt;embed src=&#34;icn2024.pdf&#34;&gt;&lt;/embed&gt;
&lt;/object&gt;</description>
    </item>
    
    <item>
      <title>Celestial compass sensor mimics the insect eye for navigation under cloudy and occluded skies</title>
      <link>https://evgkanias.github.io/publication/celestial-compass-design-mimics-the-fan-like-polarisation-filter-array-of-insect-eyes/</link>
      <pubDate>Wed, 29 Nov 2023 14:00:00 +0000</pubDate>
      <guid>https://evgkanias.github.io/publication/celestial-compass-design-mimics-the-fan-like-polarisation-filter-array-of-insect-eyes/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multimodal skylight information improves the estimation of the celestial compass: insights from a hardware implementation</title>
      <link>https://evgkanias.github.io/talk/multimodal-skylight-information-improves-the-estimation-of-the-celestial-compass-insights-from-a-hardware-implementation/</link>
      <pubDate>Tue, 22 Aug 2023 10:40:00 +0000</pubDate>
      <guid>https://evgkanias.github.io/talk/multimodal-skylight-information-improves-the-estimation-of-the-celestial-compass-insights-from-a-hardware-implementation/</guid>
      <description>&lt;iframe src=&#34;https://uoe-my.sharepoint.com/:p:/g/personal/egkanias_ed_ac_uk/EWL6VCSDjCRLvssdYn8ClLsB7dz3rjbM3y_Imx3nJv6d4g?e=XK4O4I&amp;amp;action=embedview&amp;amp;wdAr=1.7777777777777777&#34; width=&#34;660px&#34; height=&#34;395px&#34; frameborder=&#34;0&#34;&gt;
    This is an embedded &lt;a target=&#34;_blank&#34; href=&#34;https://office.com&#34;&gt;Microsoft Office&lt;/a&gt; presentation, powered by &lt;a target=&#34;_blank&#34; href=&#34;https://office.com/webapps&#34;&gt;Office Online&lt;/a&gt;.
    &lt;/iframe&gt;</description>
    </item>
    
    <item>
      <title>Memory dynamics in Drosophila&#39;s mushroom body: a computational view</title>
      <link>https://evgkanias.github.io/talk/memory-dynamics-in-drosophilas-mushroom-body-a-computational-view/</link>
      <pubDate>Sun, 28 May 2023 16:00:00 +0200</pubDate>
      <guid>https://evgkanias.github.io/talk/memory-dynamics-in-drosophilas-mushroom-body-a-computational-view/</guid>
      <description>&lt;iframe src=&#34;https://uoe-my.sharepoint.com/:p:/g/personal/egkanias_ed_ac_uk/ERlTkmG7zpFMtJ6kgm_3yqEBwxYPKNcC7hkDJsW7e0k6Hw?e=Lrgvdc&amp;amp;action=embedview&amp;amp;wdAr=1.7777777777777777&#34; width=&#34;660px&#34; height=&#34;395px&#34; frameborder=&#34;0&#34;&gt;
    This is an embedded &lt;a target=&#34;_blank&#34; href=&#34;https://office.com&#34;&gt;Microsoft Office&lt;/a&gt; presentation, powered by &lt;a target=&#34;_blank&#34; href=&#34;https://office.com/webapps&#34;&gt;Office Online&lt;/a&gt;.
    &lt;/iframe&gt;</description>
    </item>
    
    <item>
      <title>How the fan-shaped body can integrate differential familiarity for route following in desert ants</title>
      <link>https://evgkanias.github.io/talk/how-the-fan-shaped-body-can-integrate-differential-familiarity-for-route-following-in-desert-ants/</link>
      <pubDate>Tue, 11 Oct 2022 17:10:00 -0500</pubDate>
      <guid>https://evgkanias.github.io/talk/how-the-fan-shaped-body-can-integrate-differential-familiarity-for-route-following-in-desert-ants/</guid>
      <description>&lt;!--- link to video: https://uoe-my.sharepoint.com/:v:/r/personal/s1514920_ed_ac_uk/Documents/Conferences/2022-cx/cx-route-following-video.mp4?csf=1&amp;web=1&amp;e=Z2b2x6 ---&gt;
&lt;iframe src=&#34;https://uoe-my.sharepoint.com/personal/egkanias_ed_ac_uk/_layouts/15/embed.aspx?UniqueId=64d991a7-7673-463b-8289-4141716e3dd9&amp;embed=%7B%22ust%22%3Atrue%2C%22hv%22%3A%22CopyEmbedCode%22%7D&amp;referrer=StreamWebApp&amp;referrerScenario=EmbedDialog.Create&#34;
width=&#34;700&#34; height=&#34;467&#34; frameborder=&#34;0&#34; scrolling=&#34;no&#34; allowfullscreen title=&#34;cx-route-following-video.mp4&#34;&gt;&lt;/iframe&gt;
&lt;object data=&#34;cx2022.pdf&#34; type=&#34;application/pdf&#34; width=&#34;700px&#34; height=&#34;1020px&#34;&gt;
&lt;embed src=&#34;cx2022.pdf&#34;&gt;&lt;/embed&gt;
&lt;/object&gt;</description>
    </item>
    
    <item>
      <title>How could the mushroom body and central complex combine for visual homing in insects?</title>
      <link>https://evgkanias.github.io/talk/how-could-the-mushroom-body-and-central-complex-combine-for-visual-homing-in-insects/</link>
      <pubDate>Mon, 25 Jul 2022 15:00:00 +0000</pubDate>
      <guid>https://evgkanias.github.io/talk/how-could-the-mushroom-body-and-central-complex-combine-for-visual-homing-in-insects/</guid>
      <description>&lt;object data=&#34;icn2022.pdf&#34; type=&#34;application/pdf&#34; width=&#34;700px&#34; height=&#34;1020px&#34;&gt;
&lt;embed src=&#34;icn2022.pdf&#34;&gt;&lt;/embed&gt;
&lt;/object&gt;</description>
    </item>
    
    <item>
      <title>Insect-brain inspired neuromorphic nanophotonics</title>
      <link>https://evgkanias.github.io/project/insect-neuro-nano/</link>
      <pubDate>Wed, 01 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://evgkanias.github.io/project/insect-neuro-nano/</guid>
      <description>&lt;p&gt;The goal of the project is to develop nanophotonic on-chip devices for integrated sensing and neural computation, inspired by the insect brain.
This will uniquely combine four lines of research:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Progress in understanding insect neurobiology that provides proven circuit designs to solve significant problems such as autonomous navigation;&lt;/li&gt;
&lt;li&gt;Advanced III-V semiconductor nanowire technology that exploits light to obtain a large number of interconnects with extremely low power consumption;&lt;/li&gt;
&lt;li&gt;Optically efficient stable molecular dyes that can be used for novel memory components;&lt;/li&gt;
&lt;li&gt;Circuit technology developed for quantum computing.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As proof of concept, we target the complete pathway from polarised light sensing in the insect eye to the internal compass and memory circuits by which this information is integrated in a continuous accurate estimate of location.&lt;/p&gt;
&lt;p&gt;The project is funded by the EC in the Horizon Europe programme (GA 101046790). The project started on 1st of April 2022 and run for four years.&lt;/p&gt;
&lt;p&gt;The project is a collaboration of research groups at Lund University, the University of Copenhagen, the University of Edinburgh, and University of Groningen.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An anatomically accurate circuit for short- and long-term motivational learning in fruit flies</title>
      <link>https://evgkanias.github.io/talk/an-anatomically-accurate-circuit-for-short-and-long-term-motivational-learning-in-fruit-flies/</link>
      <pubDate>Sat, 19 Mar 2022 20:30:00 +0000</pubDate>
      <guid>https://evgkanias.github.io/talk/an-anatomically-accurate-circuit-for-short-and-long-term-motivational-learning-in-fruit-flies/</guid>
      <description>&lt;div align=&#34;center&#34;&gt;
&lt;iframe src=&#34;https://uoe-my.sharepoint.com/:p:/g/personal/s1514920_ed_ac_uk/ERMt5PfGJbVPn7Kav1_ejOMBJeqVY9wHIjE3wSfjBqC0wA?e=ThMT2Z&amp;amp;action=embedview&amp;amp;wdAr=1.7777777777777777&#34; width=&#34;700px&#34; height=&#34;420px&#34; frameborder=&#34;0&#34;&gt;
This is an embedded &lt;a target=&#34;_blank&#34; href=&#34;https://office.com&#34;&gt;Microsoft Office&lt;/a&gt; presentation, powered by &lt;a target=&#34;_blank&#34; href=&#34;https://office.com/webapps&#34;&gt;Office Online&lt;/a&gt;.
&lt;/iframe&gt;
&lt;object data=&#34;cosyne2022.pdf&#34; type=&#34;application/pdf&#34; width=&#34;700px&#34; height=&#34;1020px&#34;&gt;
&lt;embed src=&#34;cosyne2022.pdf&#34;&gt;&lt;/embed&gt;
&lt;/object&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>An incentive circuit for memory dynamics in the mushroom body of Drosophila melanogaster</title>
      <link>https://evgkanias.github.io/publication/an-incentive-circuit-memory-dynamics-in-the-mushroom-body-of-drosophila-melanogaster/</link>
      <pubDate>Tue, 08 Mar 2022 14:00:00 +0000</pubDate>
      <guid>https://evgkanias.github.io/publication/an-incentive-circuit-memory-dynamics-in-the-mushroom-body-of-drosophila-melanogaster/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The incentive circuit of the fruit fly brain: a computational perspective</title>
      <link>https://evgkanias.github.io/talk/the-incentive-circuit-of-the-fruit-fly-brain-a-computational-perspective/</link>
      <pubDate>Wed, 22 Sep 2021 17:00:00 +0000</pubDate>
      <guid>https://evgkanias.github.io/talk/the-incentive-circuit-of-the-fruit-fly-brain-a-computational-perspective/</guid>
      <description>&lt;object data=&#34;the-incentive-circuit-of-the-fruitfly-brain-a-computational-perspective.pdf&#34; type=&#34;application/pdf&#34; width=&#34;700px&#34; height=&#34;1020px&#34;&gt;
&lt;embed src=&#34;the-incentive-circuit-of-the-fruitfly-brain-a-computational-perspective.pdf&#34;&gt;&lt;/embed&gt;
&lt;/object&gt;</description>
    </item>
    
    <item>
      <title>How flies acquire, forget and assimilate memories: a computational perspective</title>
      <link>https://evgkanias.github.io/talk/how-flies-acquire-forget-and-assimilate-memories-a-computational-perspective/</link>
      <pubDate>Tue, 01 Jun 2021 15:32:00 +0000</pubDate>
      <guid>https://evgkanias.github.io/talk/how-flies-acquire-forget-and-assimilate-memories-a-computational-perspective/</guid>
      <description>&lt;iframe src=&#34;https://uoe-my.sharepoint.com/:p:/g/personal/egkanias_ed_ac_uk/EegYTbdFMuJFpLrs1Gm7CfgBgkI3aI1-YRDSGHWI9kgRmg?e=oNcKCT&amp;amp;action=embedview&amp;amp;wdAr=1.7777777777777777&#34; width=&#34;660px&#34; height=&#34;395px&#34; frameborder=&#34;0&#34;&gt;
This is an embedded &lt;a target=&#34;_blank&#34; href=&#34;https://office.com&#34;&gt;Microsoft Office&lt;/a&gt; presentation, powered by &lt;a target=&#34;_blank&#34; href=&#34;https://office.com/webapps&#34;&gt;Office Online&lt;/a&gt;.
&lt;/iframe&gt;</description>
    </item>
    
    <item>
      <title>How do backward-walking ants (Cataglyphis velox) cope with navigational uncertainty?</title>
      <link>https://evgkanias.github.io/publication/how-do-backward-walking-ants-cataglyphis-velox-cope-with-navigational-uncertainty/</link>
      <pubDate>Sat, 20 Jun 2020 13:00:50 +0000</pubDate>
      <guid>https://evgkanias.github.io/publication/how-do-backward-walking-ants-cataglyphis-velox-cope-with-navigational-uncertainty/</guid>
      <description></description>
    </item>
    
    <item>
      <title>From skylight input to behavioural output: a computational model of the insect polarised light compass</title>
      <link>https://evgkanias.github.io/talk/from-skylight-input-to-behavioural-output-a-computational-model-of-the-insect-polarised-light-compass/</link>
      <pubDate>Tue, 19 Nov 2019 15:50:25 +0000</pubDate>
      <guid>https://evgkanias.github.io/talk/from-skylight-input-to-behavioural-output-a-computational-model-of-the-insect-polarised-light-compass/</guid>
      <description>&lt;iframe src=&#34;https://uoe-my.sharepoint.com/:p:/g/personal/egkanias_ed_ac_uk/EYJlTlUnpjxIuJDqIVPhregBxy8ukpqqY7Aq_Tk9urPVrQ?e=tpNtgO&amp;amp;action=embedview&amp;amp;wdAr=1.7777777777777777&#34; width=&#34;660px&#34; height=&#34;395px&#34; frameborder=&#34;0&#34;&gt;
    This is an embedded &lt;a target=&#34;_blank&#34; href=&#34;https://office.com&#34;&gt;Microsoft Office&lt;/a&gt; presentation, powered by &lt;a target=&#34;_blank&#34; href=&#34;https://office.com/webapps&#34;&gt;Office Online&lt;/a&gt;.
    &lt;/iframe&gt;</description>
    </item>
    
    <item>
      <title>Robustness of a model of the insects&#39; celestial compass in realistic conditions</title>
      <link>https://evgkanias.github.io/talk/robustness-of-a-model-of-the-insects-celestial-compass-in-realistic-conditions/</link>
      <pubDate>Thu, 08 Aug 2019 16:00:00 +0000</pubDate>
      <guid>https://evgkanias.github.io/talk/robustness-of-a-model-of-the-insects-celestial-compass-in-realistic-conditions/</guid>
      <description>&lt;object data=&#34;robustness-of-a-model-of-the-insects-celestial-compass-in-realistic-conditions.pdf&#34; type=&#34;application/pdf&#34; width=&#34;700px&#34; height=&#34;1020px&#34;&gt;
&lt;embed src=&#34;robustness-of-a-model-of-the-insects-celestial-compass-in-realistic-conditions.pdf&#34;&gt;&lt;/embed&gt;
&lt;/object&gt;</description>
    </item>
    
    <item>
      <title>From skylight input to behavioural output: a computational model of the insect polarised light compass</title>
      <link>https://evgkanias.github.io/publication/from-skylight-input-to-behavioural-output-a-computational-model-of-the-insect-polarised-light-compass/</link>
      <pubDate>Thu, 18 Jul 2019 13:00:25 +0000</pubDate>
      <guid>https://evgkanias.github.io/publication/from-skylight-input-to-behavioural-output-a-computational-model-of-the-insect-polarised-light-compass/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://evgkanias.github.io/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://evgkanias.github.io/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-wowchemy&#34;&gt;Create slides in Markdown with Wowchemy&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wowchemy&lt;/a&gt; | &lt;a href=&#34;https://owchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;span class=&#34;fragment &#34; &gt;
   One 
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   **Two** 
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   Three 
&lt;/span&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/media/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-modules/discussions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Imitating the Drosophila Larval Learning Behaviour on a Robot</title>
      <link>https://evgkanias.github.io/talk/imitating-the-drosophila-larval-learning-behaviour-on-a-robot/</link>
      <pubDate>Mon, 08 Oct 2018 14:00:00 +0000</pubDate>
      <guid>https://evgkanias.github.io/talk/imitating-the-drosophila-larval-learning-behaviour-on-a-robot/</guid>
      <description>&lt;object data=&#34;imitating-the-drosophila-larval-learning-behaviour-on-a-robot.pdf&#34; type=&#34;application/pdf&#34; width=&#34;700px&#34; height=&#34;550px&#34;&gt;
&lt;embed src=&#34;imitating-the-drosophila-larval-learning-behaviour-on-a-robot.pdf&#34;&gt;&lt;/embed&gt;
&lt;/object&gt;</description>
    </item>
    
    <item>
      <title>Insect neuroethology of reinforcement learning</title>
      <link>https://evgkanias.github.io/project/phd-thesis/</link>
      <pubDate>Sat, 01 Sep 2018 00:00:00 +0000</pubDate>
      <guid>https://evgkanias.github.io/project/phd-thesis/</guid>
      <description>&lt;p&gt;Historically, reinforcement learning is a branch of machine learning founded on observations of how animals learn. This involved collaboration between the fields of biology and artificial intelligence that was beneficial to both fields, creating smarter artificial agents and improving the understanding of how biological systems function. The evolution of reinforcement learning during the past few years was rapid but substantially diverged from providing insights into how biological systems work, opening a gap between reinforcement learning and biology. In an attempt to close this gap, this thesis studied the insect neuroethology of reinforcement learning, that is, the neural circuits that underlie reinforcement-learning-related behaviours in insects. The goal was to extract a biologically plausible plasticity function from insect-neuronal data, use this to explain biological findings and compare it to more standard reinforcement learning models.
Consequently, a novel dopaminergic plasticity rule was developed to approximate the function of dopamine as the plasticity mechanism between neurons in the insect brain. This allowed a range of observed learning phenomena to happen in parallel, like memory depression, potentiation, recovery, and saturation. In addition, by using anatomical data of connections between neurons in the mushroom body neuropils of the insect brain, the neural incentive circuit of dopaminergic and output neurons was also explored. This, together with the dopaminergic plasticity rule, allowed for dynamic collaboration amongst parallel memory functions, such as acquisition, transfer, and forgetting. When tested on olfactory conditioning paradigms, the model reproduced the observed changes in the activity of the identified neurons in fruit flies. It also replicated the observed behaviour of the animals and it allowed for flexible behavioural control. Inspired by the visual navigation system of desert ants, the model was further challenged in the visual place recognition task. Although a relatively simple encoding of the olfactory information was sufficient to explain odour learning, a more sophisticated encoding of the visual input was required to increase the separability among the visual inputs and enable visual place recognition. Signal whitening and sparse combinatorial encoding were sufficient to boost the performance of the system in this task. The incentive circuit enabled the encoding of increasing familiarity along a known route, which dropped proportionally to the distance of the animal from that route. Finally, the proposed model was challenged in delayed reinforcement tasks, suggesting that
it might take the role of an adaptive critic in the context of reinforcement learning.&lt;/p&gt;
&lt;iframe src=&#34;https://uoe-my.sharepoint.com/:p:/g/personal/s1514920_ed_ac_uk/EUsmuoxPFbNIlMwqnA1bfFwB583vmXC3xVBWlFZbKBjtWA?e=GA6CDE&amp;amp;action=embedview&amp;amp;wdAr=1.7777777777777777&#34; width=&#34;660px&#34; height=&#34;395px&#34; frameborder=&#34;0&#34;&gt;
This is an embedded &lt;a target=&#34;_blank&#34; href=&#34;https://office.com&#34;&gt;Microsoft Office&lt;/a&gt; presentation, powered by &lt;a target=&#34;_blank&#34; href=&#34;https://office.com/webapps&#34;&gt;Office Online&lt;/a&gt;.
&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>Neural models of ant navigation in a realistic 3D world</title>
      <link>https://evgkanias.github.io/talk/neural-models-of-ant-navigation-in-a-realistic-3d-world/</link>
      <pubDate>Sun, 22 Jul 2018 10:00:00 +0000</pubDate>
      <guid>https://evgkanias.github.io/talk/neural-models-of-ant-navigation-in-a-realistic-3d-world/</guid>
      <description>&lt;object data=&#34;neural-models-of-ant-navigation-in-a-realistic-3d-world.pdf&#34; type=&#34;application/pdf&#34; width=&#34;700px&#34; height=&#34;1050px&#34;&gt;
&lt;embed src=&#34;neural-models-of-ant-navigation-in-a-realistic-3d-world.pdf&#34;&gt;&lt;/embed&gt;
&lt;/object&gt;</description>
    </item>
    
    <item>
      <title>Predator Evasion by a Robocrab</title>
      <link>https://evgkanias.github.io/talk/predator-evasion-by-a-robocrab/</link>
      <pubDate>Thu, 27 Jul 2017 10:00:09 +0000</pubDate>
      <guid>https://evgkanias.github.io/talk/predator-evasion-by-a-robocrab/</guid>
      <description>&lt;iframe src=&#34;https://uoe-my.sharepoint.com/:p:/g/personal/egkanias_ed_ac_uk/EXJXQfYJYUtLkq_JtR03zYYBtknESSPE00mawmYulovEEw?e=f8oYiT&amp;amp;action=embedview&amp;amp;wdAr=1.7777777777777777&#34; width=&#34;660px&#34; height=&#34;395px&#34; frameborder=&#34;0&#34;&gt;
This is an embedded &lt;a target=&#34;_blank&#34; href=&#34;https://office.com&#34;&gt;Microsoft Office&lt;/a&gt; presentation, powered by &lt;a target=&#34;_blank&#34; href=&#34;https://office.com/webapps&#34;&gt;Office Online&lt;/a&gt;.
&lt;/iframe&gt;</description>
    </item>
    
    <item>
      <title>Predator Evasion by a Robocrab</title>
      <link>https://evgkanias.github.io/publication/predator-evasion-by-a-robocrab/</link>
      <pubDate>Sun, 16 Jul 2017 13:00:51 +0000</pubDate>
      <guid>https://evgkanias.github.io/publication/predator-evasion-by-a-robocrab/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Exploiting invisible cues for robot navigation in complex natural environments</title>
      <link>https://evgkanias.github.io/project/invisible-cues/</link>
      <pubDate>Wed, 01 Mar 2017 00:00:00 +0000</pubDate>
      <guid>https://evgkanias.github.io/project/invisible-cues/</guid>
      <description>&lt;p&gt;Outdoor navigation in natural environments remains a challenge for robotics.
Recent breakthroughs in robot navigation have been dependent on specific sensor
technologies, such as laser depth sensors and GPS, and advanced image processing.
The ability of animals such as ants to navigate effectively without such
power- and computation- hungry systems are a proof of principle that
alternative cheaper approaches are viable. Ants also have specialised sensing,
with a peripheral visual system that has evolved to be sensitive to crucial
cues for navigation. Specifically, they make use of non-visible (to humans)
light cues in the form of ultraviolet (UV) and polarised light detection.
UV detection allows the important signal of the horizon shape against the sky
to be easily distinguished. Polarised light detection provides an external
compass cue of heading relative to the sun direction, even when only a small
portion of the sky is visible.&lt;/p&gt;
&lt;p&gt;We propose to build a sensory system that gathers the full range of light
cues available to the ant, in its natural ecological situation, and to analyse
the information contained in this signal. We will also analyse how the
specific sensor layout (ommatidia array), peripheral receptor characteristics,
and the motor behaviour of the ant may contribute to extracting salient
information. The data will form a test-bed for comparison of algorithmic and
neural models of the processing that underlies the navigation capabilities
of the ant. To date, these cues have been considered separately but we believe
the navigational success of this system depends on the specific combination.
For example, the directional information in the polarised sky may form an
important part of visual memories; and UV information may contribute to
disambiguation of the polarisation pattern and the robustness of this
information under different cloud conditions.&lt;/p&gt;
&lt;p&gt;There has been a substantial increase in the last few years in research into
insect neural pathways involved in processing these cues which has yet to be
exploited in robot models. In particular there has been breakthrough work on
the central brain mechanisms involved in decoding polarised light to obtain
heading direction. There is also a rapidly increasing understanding of the
circuits involved in learning, a key component of navigation capabilities.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Miniature insect model for active learning (minimal)</title>
      <link>https://evgkanias.github.io/project/minimal/</link>
      <pubDate>Thu, 01 Sep 2016 00:00:00 +0000</pubDate>
      <guid>https://evgkanias.github.io/project/minimal/</guid>
      <description>&lt;p&gt;Biology provides the inspiration for a vision of small low-power devices that are able to learn rapidly and autonomously about environmental contingencies, enabling prediction and adaptive anticipatory action. Larval Drosophila have fewer than 10,000 neurons, yet express a variety of complex orientation and learning behaviours, including non-trivial anticipatory actions requiring context-dependent evaluation of the value of learned cues. Current computational learning theory cannot fully account for or replicate these capacities. We aim to develop a new foundation for understanding natural learning by developing a complete multilevel model of learning in larvae.&lt;/p&gt;
&lt;p&gt;Our aims are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;to analyse at a fine scale how larval olfactory behaviour is controlled and altered by associative conditioning, linked to agent-based models that ground learning capabilities in ongoing sensorimotor control;&lt;/li&gt;
&lt;li&gt;to build one-to-one computational neural models that can be validated by exploiting the recent expansion of the Drosophila neurogenetic toolkit to gain unprecedented ability to characterise and manipulate neural circuits during unconstrained behaviour;&lt;/li&gt;
&lt;li&gt;to derive from these models novel, generalisable algorithms and circuit architectures that can be used to enhance the learning and anticipatory capabilities of machines.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Robocrab: data-driven adaptation of the evation behaviour in fiddler crabs</title>
      <link>https://evgkanias.github.io/project/msc-thesis/</link>
      <pubDate>Mon, 01 Feb 2016 00:00:00 +0000</pubDate>
      <guid>https://evgkanias.github.io/project/msc-thesis/</guid>
      <description>&lt;p&gt;Analysing the behaviour of animals and studying their brain structure is a common
way to create bio-inspired artificial intelligence methods. Fiddler crabs are animals
with specific limitations on their sensors that have a great ability of evading on the
presence of potential predators. In this project we study these animals with respect to
the physiology of their unique sensor – their vision – and their evading manoeuvres,
in order to create a machine learning model, which imitates this interesting behaviour.
More specifically, we propose a semi-supervised architecture of neural network, inspired
by the structure of fiddler crabs’ brain and their evasion behaviour’s feedback
loops. We combine location specific visual units and LSTM recurrent units in order to
build a model capable to adapt this behaviour. We trained our model using a data-set
we created using data from experiments done with living crabs in their habitat. Comparing
our statistical results of our model’s behaviour with the ones of the original
crabs behaviour we show that this model captured some key features of this behaviour
as well as it seems to behave quite realistic in the simulations. Therefore, we show that
it is reasonable to consider machine learning models as potential solutions in animal
behaviour adaptation problems.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>FI-STAR</title>
      <link>https://evgkanias.github.io/project/fi-star/</link>
      <pubDate>Mon, 31 Aug 2015 14:00:00 +0000</pubDate>
      <guid>https://evgkanias.github.io/project/fi-star/</guid>
      <description>&lt;p&gt;FI-STAR will establish early trials in the Health Care domain building on Future Internet (FI) technology leveraging on the outcomes of FI-PPP Phase 1.&lt;/p&gt;
&lt;p&gt;It will become self-sufficient after the end of the project and will continue on a sustainable business model by several partners. In order to meet the requirements of a global Health industry FI-STAR will use a fundamentally different, “reverse” cloud approach that is. It will bring the software to the data, rather than bringing the data to the software. FI-STAR will create a robust framework based of the “software to data” paradigm.&lt;/p&gt;
&lt;p&gt;A sustainable value chain following the life cycle of the Generic Enablers (GEs) will enable FI-STAR to grow beyond the lifetime of the project. FI-STAR will build a vertical community in order to create a sustainable ecosystem for all user groups in the global Health care and adjacent markets based on FI-PPP specifications.&lt;/p&gt;
&lt;p&gt;FI-STAR will deploy and execute 7 early trials across Europe, serving more than 4 million people. Through the trials FI-STAR will validate the FI-PPP core platform concept by using GEs to build its framework and will introduce ultra-light interactive applications for user functionality.&lt;/p&gt;
&lt;p&gt;It will pro-actively engage with the FI-PPP to propose specifications and standards.FI-STAR will use the latest digital media technology for community building and will proactively prepare for Phase 3 through targeted elicitation of new partners using open calls.&lt;/p&gt;
&lt;p&gt;Finally, FI-STAR will collaborate with other FI-PPP projects, through the mechanisms in place, by actively interacting with all necessary bodies. FI-STAR is a unique opportunity for implementing Future Internet Private-Public Partnership in the Health Care domain, by offering to the community standardized and certified software including a safe, secure and resilient platform, taking advantage of all Cloud Computing benefits and guaranteeing the protection of sensitive and personal data travelling in Public Clouds.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Replay</title>
      <link>https://evgkanias.github.io/project/replay/</link>
      <pubDate>Mon, 31 Aug 2015 14:00:00 +0000</pubDate>
      <guid>https://evgkanias.github.io/project/replay/</guid>
      <description>&lt;p&gt;The RePlay project has as its goal the development of a technology platform
that shall provide access and interpretation of digital content for Traditional
Sports and Games (TSG). It will enable multiple modes of training, coaching and
knowledge sharing that will contribute to the increased participation and preservation
of traditional sport in the future. This will be achieved by developing a base
technology and methodologies for the digitisation of the art and forms of play of a
set of representative sports. In the case of RePlay, this will be field based
Gaelic team sports and Basque individual/doubles ball and court sports.
The fundamental structure of these sports is extensible to a vast majority of traditional
minority sports and mainstream sports.&lt;/p&gt;
&lt;p&gt;RePlay will consist of the design and implementation of a platform for the capture, annotation, indexing and provision of 3D sports content. It will include the analysis and specification of methodologies and ideal cost-effective hardware solutions for the extension of the project to other sports. The project will focus on the use of existing and near future 3D motion capture hardware. The project does not include the development of any hardware as an objective, as the market is already addressing this. It shall instead focus on the creation of the knowledge and underlying software tools that will provide a low-cost entry point to other TSG associations.&lt;/p&gt;
&lt;p&gt;RePlay will focus on the analysis, capture and modelling of the basic styles and techniques of play common to all participants or the &amp;ldquo;Local Hero&amp;rdquo; using low-cost capture techniques. However, RePlay will also use advanced professional grade capture techniques on &amp;ldquo;National Heroes&amp;rdquo;. A national hero, or a recognised elite player, develops their sporting prowess to an extent that is unique. This presents Intangible Cultural Heritage to be preserved, an opportunity to allow the young to try to learn and emulate their heroes, and a scientific opportunity to compare and analyse the evolution in the changes of styles of play over time. RePlay will also include work on retrospective analysis of sports via video content.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep learning algorithms for multi-label data</title>
      <link>https://evgkanias.github.io/project/bsc-thesis/</link>
      <pubDate>Fri, 01 Feb 2013 00:00:00 +0000</pubDate>
      <guid>https://evgkanias.github.io/project/bsc-thesis/</guid>
      <description>&lt;p&gt;Many scientists, who specialize in supervised learning, are interested in multi-label
data for two main reasons. First of all, this type of data arising at a variety of
applications such as semantic indexing of documents, music and video, protein
function prediction, medical diagnosis, drug discovery and search engine queries
clustering. Secondly, multi-label data perform interesting research challenges like the
use of correlations between labels and scaling to a large number of labels.&lt;/p&gt;
&lt;p&gt;In multi-label data, every subject of interest is characterized by one or more labels
from a label set and the purpose is not a simple classification of an instance, but a
label ranking for their relevance to the subject or a bipartition of them to these that are
relevant and those which are not (multi-label classification). In this project, we
investigate this field and we apply a deep learning method, inspired by the theory that
explains how the brain recognizes patterns. Technology companies are reporting
startling gains in fields as diverse as computer vision, speech recognition and the
identification of promising new molecules for designing drugs.&lt;/p&gt;
&lt;p&gt;We present a fairly recent kind of neural network, the deep belief network, which
handles multi-label data without transforming them before or after the training. This
transformation is undesirable, because it causes the dataset to be bigger. Our aim is to
build a model that can handle multi-label data using deep learning techniques. In this
project, we showed that these techniques – specifically the deep learning using belief
networks – have better results in the most of the encountered subjects of interest&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://evgkanias.github.io/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://evgkanias.github.io/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
