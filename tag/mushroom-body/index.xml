<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>mushroom body | Evripidis Gkanias</title>
    <link>https://evgkanias.github.io/tag/mushroom-body/</link>
      <atom:link href="https://evgkanias.github.io/tag/mushroom-body/index.xml" rel="self" type="application/rss+xml" />
    <description>mushroom body</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-gb</language><copyright>Â© 2022 Evripidis Gkanias</copyright><lastBuildDate>Tue, 08 Mar 2022 14:00:00 +0000</lastBuildDate>
    <image>
      <url>https://evgkanias.github.io/media/icon_hu22afa8837ab305f0f3e5e2b9e7b7c4db_6290075_512x512_fill_lanczos_center_3.png</url>
      <title>mushroom body</title>
      <link>https://evgkanias.github.io/tag/mushroom-body/</link>
    </image>
    
    <item>
      <title>An incentive circuit for memory dynamics in the mushroom body of Drosophila melanogaster</title>
      <link>https://evgkanias.github.io/publication/the-incentive-circuit-memory-dynamics-in-the-mushroom-body-of-drosophila-melanogaster/</link>
      <pubDate>Tue, 08 Mar 2022 14:00:00 +0000</pubDate>
      <guid>https://evgkanias.github.io/publication/the-incentive-circuit-memory-dynamics-in-the-mushroom-body-of-drosophila-melanogaster/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Insect neuroethology of reinforcement learning</title>
      <link>https://evgkanias.github.io/project/phd-thesis/</link>
      <pubDate>Sat, 01 Sep 2018 00:00:00 +0000</pubDate>
      <guid>https://evgkanias.github.io/project/phd-thesis/</guid>
      <description>&lt;p&gt;Historically, reinforcement learning is a branch of machine learning founded on observations of how biological systems,
such as animals, learn to survive in their environment. This enabled collaboration between the fields of biology and artificial intelligence,
which aimed to benefit both by creating smarter artificial agents and improving our understanding of how biological systems function.
The evolution of reinforcement learning during the past few years is rapid, which mostly built on the advantages of deep learning and artificial neural networks,
allowed computational models to outperform humans in certain tasks, like playing board and video games, solving complicated puzzles, and controlling robotic parts.
These led to breakthroughs in artificial intelligence that massively increased the popularity of reinforcement learning as a tool to optimise the
parameters of computational models. However, these successful but rather complicated models can no longer provide any insights into how biological systems work.
In order to bridge the gap between reinforcement learning and biology, at least to some extent, we have decided to study the neuroethology of reinforcement learning,
focusing on the insect brain. The term neuroethology refers to the neural basis of natural behaviour in animals. Therefore, our goal is to extract a biologically plausible
plasticity function from insect-neuroscience data, use them to explain biological findings and provide insights on how this function can be incorporated into more
standard reinforcement learning.&lt;/p&gt;
&lt;p&gt;More specifically, we studied the function of dopamine as the plasticity mechanism between neurons in the insect brain and developed our own dopaminergic plasticity rule
that allows a range of observed learning phenomena to happen in parallel. Using anatomical data of connections between neurons in the same brain regions,
we also develop a neural circuit of dopaminergic and output neurons (related to actions in the reinforcement learning context), which together with the dopaminergic
plasticity rule allows complicated memory dynamics: acquiring, transferring and forgetting memories based on both internal and external signals.
The model can reproduce the observed changes in the activity of each of the identified neurons in olfactory conditioning paradigms, replicate the observed behaviour
of the animals, and allow for flexible behavioural control. We further challenged our model in visual place recognition.
Although a relatively simple encoding of the olfactory information is sufficient in order to explain the observed behaviour,
we found that the encoding of the visual input must be more sophisticated, simultaneously reducing the correlation among the extracted features,
and increasing the number of features and sparsity of the feature vector. A combination of signal whitening, based on the principle component analysis,
and a heuristic combinatorial approach, was sufficient to boost the performance of our model in the visual place recognition task and create useful correlations among
the extracted features. Moreover, we showed that the memory dynamics (which emerged from our circuit) enable increasing certainty when familiar places are presented in a
sequence but not necessarily in the correct order. Finally, we challenged our model with classic reinforcement learning benchmarks, and suggest that spatial-correlated
features are less useful than time-correlated ones.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Miniature insect model for active learning (minimal)</title>
      <link>https://evgkanias.github.io/project/minimal/</link>
      <pubDate>Thu, 01 Sep 2016 00:00:00 +0000</pubDate>
      <guid>https://evgkanias.github.io/project/minimal/</guid>
      <description>&lt;p&gt;Biology provides the inspiration for a vision of small low-power devices that are able to learn rapidly and autonomously about environmental contingencies, enabling prediction and adaptive anticipatory action. Larval Drosophila have fewer than 10,000 neurons, yet express a variety of complex orientation and learning behaviours, including non-trivial anticipatory actions requiring context-dependent evaluation of the value of learned cues. Current computational learning theory cannot fully account for or replicate these capacities. We aim to develop a new foundation for understanding natural learning by developing a complete multilevel model of learning in larvae.&lt;/p&gt;
&lt;p&gt;Our aims are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;to analyse at a fine scale how larval olfactory behaviour is controlled and altered by associative conditioning, linked to agent-based models that ground learning capabilities in ongoing sensorimotor control;&lt;/li&gt;
&lt;li&gt;to build one-to-one computational neural models that can be validated by exploiting the recent expansion of the Drosophila neurogenetic toolkit to gain unprecedented ability to characterise and manipulate neural circuits during unconstrained behaviour;&lt;/li&gt;
&lt;li&gt;to derive from these models novel, generalisable algorithms and circuit architectures that can be used to enhance the learning and anticipatory capabilities of machines.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
