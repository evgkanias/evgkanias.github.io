<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>behaviour | Evripidis Gkanias</title>
    <link>https://evgkanias.github.io/tag/behaviour/</link>
      <atom:link href="https://evgkanias.github.io/tag/behaviour/index.xml" rel="self" type="application/rss+xml" />
    <description>behaviour</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-gb</language><copyright>© 2024 Evripidis Gkanias</copyright><lastBuildDate>Tue, 11 Oct 2022 17:10:00 -0500</lastBuildDate>
    <image>
      <url>https://evgkanias.github.io/media/icon_hu22afa8837ab305f0f3e5e2b9e7b7c4db_6290075_512x512_fill_lanczos_center_3.png</url>
      <title>behaviour</title>
      <link>https://evgkanias.github.io/tag/behaviour/</link>
    </image>
    
    <item>
      <title>How the fan-shaped body can integrate differential familiarity for route following in desert ants</title>
      <link>https://evgkanias.github.io/talk/how-the-fan-shaped-body-can-integrate-differential-familiarity-for-route-following-in-desert-ants/</link>
      <pubDate>Tue, 11 Oct 2022 17:10:00 -0500</pubDate>
      <guid>https://evgkanias.github.io/talk/how-the-fan-shaped-body-can-integrate-differential-familiarity-for-route-following-in-desert-ants/</guid>
      <description>&lt;!--- link to video: https://uoe-my.sharepoint.com/:v:/r/personal/s1514920_ed_ac_uk/Documents/Conferences/2022-cx/cx-route-following-video.mp4?csf=1&amp;web=1&amp;e=Z2b2x6 ---&gt;
&lt;iframe src=&#34;https://uoe-my.sharepoint.com/personal/egkanias_ed_ac_uk/_layouts/15/embed.aspx?UniqueId=64d991a7-7673-463b-8289-4141716e3dd9&amp;embed=%7B%22ust%22%3Atrue%2C%22hv%22%3A%22CopyEmbedCode%22%7D&amp;referrer=StreamWebApp&amp;referrerScenario=EmbedDialog.Create&#34;
width=&#34;700&#34; height=&#34;467&#34; frameborder=&#34;0&#34; scrolling=&#34;no&#34; allowfullscreen title=&#34;cx-route-following-video.mp4&#34;&gt;&lt;/iframe&gt;
&lt;object data=&#34;cx2022.pdf&#34; type=&#34;application/pdf&#34; width=&#34;700px&#34; height=&#34;1020px&#34;&gt;
&lt;embed src=&#34;cx2022.pdf&#34;&gt;&lt;/embed&gt;
&lt;/object&gt;</description>
    </item>
    
    <item>
      <title>An incentive circuit for memory dynamics in the mushroom body of Drosophila melanogaster</title>
      <link>https://evgkanias.github.io/publication/an-incentive-circuit-memory-dynamics-in-the-mushroom-body-of-drosophila-melanogaster/</link>
      <pubDate>Tue, 08 Mar 2022 14:00:00 +0000</pubDate>
      <guid>https://evgkanias.github.io/publication/an-incentive-circuit-memory-dynamics-in-the-mushroom-body-of-drosophila-melanogaster/</guid>
      <description></description>
    </item>
    
    <item>
      <title>How do backward-walking ants (Cataglyphis velox) cope with navigational uncertainty?</title>
      <link>https://evgkanias.github.io/publication/how-do-backward-walking-ants-cataglyphis-velox-cope-with-navigational-uncertainty/</link>
      <pubDate>Sat, 20 Jun 2020 13:00:50 +0000</pubDate>
      <guid>https://evgkanias.github.io/publication/how-do-backward-walking-ants-cataglyphis-velox-cope-with-navigational-uncertainty/</guid>
      <description></description>
    </item>
    
    <item>
      <title>From skylight input to behavioural output: a computational model of the insect polarised light compass</title>
      <link>https://evgkanias.github.io/talk/from-skylight-input-to-behavioural-output-a-computational-model-of-the-insect-polarised-light-compass/</link>
      <pubDate>Tue, 19 Nov 2019 15:50:25 +0000</pubDate>
      <guid>https://evgkanias.github.io/talk/from-skylight-input-to-behavioural-output-a-computational-model-of-the-insect-polarised-light-compass/</guid>
      <description>&lt;iframe src=&#34;https://uoe-my.sharepoint.com/:p:/g/personal/egkanias_ed_ac_uk/EYJlTlUnpjxIuJDqIVPhregBxy8ukpqqY7Aq_Tk9urPVrQ?e=tpNtgO&amp;amp;action=embedview&amp;amp;wdAr=1.7777777777777777&#34; width=&#34;660px&#34; height=&#34;395px&#34; frameborder=&#34;0&#34;&gt;
    This is an embedded &lt;a target=&#34;_blank&#34; href=&#34;https://office.com&#34;&gt;Microsoft Office&lt;/a&gt; presentation, powered by &lt;a target=&#34;_blank&#34; href=&#34;https://office.com/webapps&#34;&gt;Office Online&lt;/a&gt;.
    &lt;/iframe&gt;</description>
    </item>
    
    <item>
      <title>From skylight input to behavioural output: a computational model of the insect polarised light compass</title>
      <link>https://evgkanias.github.io/publication/from-skylight-input-to-behavioural-output-a-computational-model-of-the-insect-polarised-light-compass/</link>
      <pubDate>Thu, 18 Jul 2019 13:00:25 +0000</pubDate>
      <guid>https://evgkanias.github.io/publication/from-skylight-input-to-behavioural-output-a-computational-model-of-the-insect-polarised-light-compass/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Imitating the Drosophila Larval Learning Behaviour on a Robot</title>
      <link>https://evgkanias.github.io/talk/imitating-the-drosophila-larval-learning-behaviour-on-a-robot/</link>
      <pubDate>Mon, 08 Oct 2018 14:00:00 +0000</pubDate>
      <guid>https://evgkanias.github.io/talk/imitating-the-drosophila-larval-learning-behaviour-on-a-robot/</guid>
      <description>&lt;object data=&#34;imitating-the-drosophila-larval-learning-behaviour-on-a-robot.pdf&#34; type=&#34;application/pdf&#34; width=&#34;700px&#34; height=&#34;550px&#34;&gt;
&lt;embed src=&#34;imitating-the-drosophila-larval-learning-behaviour-on-a-robot.pdf&#34;&gt;&lt;/embed&gt;
&lt;/object&gt;</description>
    </item>
    
    <item>
      <title>Predator Evasion by a Robocrab</title>
      <link>https://evgkanias.github.io/talk/predator-evasion-by-a-robocrab/</link>
      <pubDate>Thu, 27 Jul 2017 10:00:09 +0000</pubDate>
      <guid>https://evgkanias.github.io/talk/predator-evasion-by-a-robocrab/</guid>
      <description>&lt;iframe src=&#34;https://uoe-my.sharepoint.com/:p:/g/personal/egkanias_ed_ac_uk/EXJXQfYJYUtLkq_JtR03zYYBtknESSPE00mawmYulovEEw?e=f8oYiT&amp;amp;action=embedview&amp;amp;wdAr=1.7777777777777777&#34; width=&#34;660px&#34; height=&#34;395px&#34; frameborder=&#34;0&#34;&gt;
This is an embedded &lt;a target=&#34;_blank&#34; href=&#34;https://office.com&#34;&gt;Microsoft Office&lt;/a&gt; presentation, powered by &lt;a target=&#34;_blank&#34; href=&#34;https://office.com/webapps&#34;&gt;Office Online&lt;/a&gt;.
&lt;/iframe&gt;</description>
    </item>
    
    <item>
      <title>Predator Evasion by a Robocrab</title>
      <link>https://evgkanias.github.io/publication/predator-evasion-by-a-robocrab/</link>
      <pubDate>Sun, 16 Jul 2017 13:00:51 +0000</pubDate>
      <guid>https://evgkanias.github.io/publication/predator-evasion-by-a-robocrab/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Exploiting invisible cues for robot navigation in complex natural environments</title>
      <link>https://evgkanias.github.io/project/invisible-cues/</link>
      <pubDate>Wed, 01 Mar 2017 00:00:00 +0000</pubDate>
      <guid>https://evgkanias.github.io/project/invisible-cues/</guid>
      <description>&lt;p&gt;Outdoor navigation in natural environments remains a challenge for robotics.
Recent breakthroughs in robot navigation have been dependent on specific sensor
technologies, such as laser depth sensors and GPS, and advanced image processing.
The ability of animals such as ants to navigate effectively without such
power- and computation- hungry systems are a proof of principle that
alternative cheaper approaches are viable. Ants also have specialised sensing,
with a peripheral visual system that has evolved to be sensitive to crucial
cues for navigation. Specifically, they make use of non-visible (to humans)
light cues in the form of ultraviolet (UV) and polarised light detection.
UV detection allows the important signal of the horizon shape against the sky
to be easily distinguished. Polarised light detection provides an external
compass cue of heading relative to the sun direction, even when only a small
portion of the sky is visible.&lt;/p&gt;
&lt;p&gt;We propose to build a sensory system that gathers the full range of light
cues available to the ant, in its natural ecological situation, and to analyse
the information contained in this signal. We will also analyse how the
specific sensor layout (ommatidia array), peripheral receptor characteristics,
and the motor behaviour of the ant may contribute to extracting salient
information. The data will form a test-bed for comparison of algorithmic and
neural models of the processing that underlies the navigation capabilities
of the ant. To date, these cues have been considered separately but we believe
the navigational success of this system depends on the specific combination.
For example, the directional information in the polarised sky may form an
important part of visual memories; and UV information may contribute to
disambiguation of the polarisation pattern and the robustness of this
information under different cloud conditions.&lt;/p&gt;
&lt;p&gt;There has been a substantial increase in the last few years in research into
insect neural pathways involved in processing these cues which has yet to be
exploited in robot models. In particular there has been breakthrough work on
the central brain mechanisms involved in decoding polarised light to obtain
heading direction. There is also a rapidly increasing understanding of the
circuits involved in learning, a key component of navigation capabilities.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Miniature insect model for active learning (minimal)</title>
      <link>https://evgkanias.github.io/project/minimal/</link>
      <pubDate>Thu, 01 Sep 2016 00:00:00 +0000</pubDate>
      <guid>https://evgkanias.github.io/project/minimal/</guid>
      <description>&lt;p&gt;Biology provides the inspiration for a vision of small low-power devices that are able to learn rapidly and autonomously about environmental contingencies, enabling prediction and adaptive anticipatory action. Larval Drosophila have fewer than 10,000 neurons, yet express a variety of complex orientation and learning behaviours, including non-trivial anticipatory actions requiring context-dependent evaluation of the value of learned cues. Current computational learning theory cannot fully account for or replicate these capacities. We aim to develop a new foundation for understanding natural learning by developing a complete multilevel model of learning in larvae.&lt;/p&gt;
&lt;p&gt;Our aims are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;to analyse at a fine scale how larval olfactory behaviour is controlled and altered by associative conditioning, linked to agent-based models that ground learning capabilities in ongoing sensorimotor control;&lt;/li&gt;
&lt;li&gt;to build one-to-one computational neural models that can be validated by exploiting the recent expansion of the Drosophila neurogenetic toolkit to gain unprecedented ability to characterise and manipulate neural circuits during unconstrained behaviour;&lt;/li&gt;
&lt;li&gt;to derive from these models novel, generalisable algorithms and circuit architectures that can be used to enhance the learning and anticipatory capabilities of machines.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Robocrab: data-driven adaptation of the evation behaviour in fiddler crabs</title>
      <link>https://evgkanias.github.io/project/msc-thesis/</link>
      <pubDate>Mon, 01 Feb 2016 00:00:00 +0000</pubDate>
      <guid>https://evgkanias.github.io/project/msc-thesis/</guid>
      <description>&lt;p&gt;Analysing the behaviour of animals and studying their brain structure is a common
way to create bio-inspired artificial intelligence methods. Fiddler crabs are animals
with specific limitations on their sensors that have a great ability of evading on the
presence of potential predators. In this project we study these animals with respect to
the physiology of their unique sensor – their vision – and their evading manoeuvres,
in order to create a machine learning model, which imitates this interesting behaviour.
More specifically, we propose a semi-supervised architecture of neural network, inspired
by the structure of fiddler crabs’ brain and their evasion behaviour’s feedback
loops. We combine location specific visual units and LSTM recurrent units in order to
build a model capable to adapt this behaviour. We trained our model using a data-set
we created using data from experiments done with living crabs in their habitat. Comparing
our statistical results of our model’s behaviour with the ones of the original
crabs behaviour we show that this model captured some key features of this behaviour
as well as it seems to behave quite realistic in the simulations. Therefore, we show that
it is reasonable to consider machine learning models as potential solutions in animal
behaviour adaptation problems.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
