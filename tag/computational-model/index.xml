<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>computational model | Evripidis Gkanias</title>
    <link>https://evgkanias.github.io/tag/computational-model/</link>
      <atom:link href="https://evgkanias.github.io/tag/computational-model/index.xml" rel="self" type="application/rss+xml" />
    <description>computational model</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-gb</language><copyright>© 2022 Evripidis Gkanias</copyright><lastBuildDate>Tue, 08 Mar 2022 14:00:00 +0000</lastBuildDate>
    <image>
      <url>https://evgkanias.github.io/media/icon_hu22afa8837ab305f0f3e5e2b9e7b7c4db_6290075_512x512_fill_lanczos_center_3.png</url>
      <title>computational model</title>
      <link>https://evgkanias.github.io/tag/computational-model/</link>
    </image>
    
    <item>
      <title>An incentive circuit for memory dynamics in the mushroom body of Drosophila melanogaster</title>
      <link>https://evgkanias.github.io/publication/the-incentive-circuit-memory-dynamics-in-the-mushroom-body-of-drosophila-melanogaster/</link>
      <pubDate>Tue, 08 Mar 2022 14:00:00 +0000</pubDate>
      <guid>https://evgkanias.github.io/publication/the-incentive-circuit-memory-dynamics-in-the-mushroom-body-of-drosophila-melanogaster/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Insect Neuroethology of Reinforcement Learning</title>
      <link>https://evgkanias.github.io/project/phd-thesis/</link>
      <pubDate>Sat, 01 Sep 2018 00:00:00 +0000</pubDate>
      <guid>https://evgkanias.github.io/project/phd-thesis/</guid>
      <description>&lt;p&gt;Historically, reinforcement learning is a branch of machine learning founded on observations of how biological systems,
such as animals, learn to survive in their environment. This enabled collaboration between the fields of biology and artificial intelligence,
which aimed to benefit both by creating smarter artificial agents and improving our understanding of how biological systems function.
The evolution of reinforcement learning during the past few years is rapid, which mostly built on the advantages of deep learning and artificial neural networks,
allowed computational models to outperform humans in certain tasks, like playing board and video games, solving complicated puzzles, and controlling robotic parts.
These led to breakthroughs in artificial intelligence that massively increased the popularity of reinforcement learning as a tool to optimise the
parameters of computational models. However, these successful but rather complicated models can no longer provide any insights into how biological systems work.
In order to bridge the gap between reinforcement learning and biology, at least to some extent, we have decided to study the neuroethology of reinforcement learning,
focusing on the insect brain. The term neuroethology refers to the neural basis of natural behaviour in animals. Therefore, our goal is to extract a biologically plausible
plasticity function from insect-neuroscience data, use them to explain biological findings and provide insights on how this function can be incorporated into more
standard reinforcement learning.&lt;/p&gt;
&lt;p&gt;More specifically, we studied the function of dopamine as the plasticity mechanism between neurons in the insect brain and developed our own dopaminergic plasticity rule
that allows a range of observed learning phenomena to happen in parallel. Using anatomical data of connections between neurons in the same brain regions,
we also develop a neural circuit of dopaminergic and output neurons (related to actions in the reinforcement learning context), which together with the dopaminergic
plasticity rule allows complicated memory dynamics: acquiring, transferring and forgetting memories based on both internal and external signals.
The model can reproduce the observed changes in the activity of each of the identified neurons in olfactory conditioning paradigms, replicate the observed behaviour
of the animals, and allow for flexible behavioural control. We further challenged our model in visual place recognition.
Although a relatively simple encoding of the olfactory information is sufficient in order to explain the observed behaviour,
we found that the encoding of the visual input must be more sophisticated, simultaneously reducing the correlation among the extracted features,
and increasing the number of features and sparsity of the feature vector. A combination of signal whitening, based on the principle component analysis,
and a heuristic combinatorial approach, was sufficient to boost the performance of our model in the visual place recognition task and create useful correlations among
the extracted features. Moreover, we showed that the memory dynamics (which emerged from our circuit) enable increasing certainty when familiar places are presented in a
sequence but not necessarily in the correct order. Finally, we challenged our model with classic reinforcement learning benchmarks, and suggest that spatial-correlated
features are less useful than time-correlated ones.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Exploiting invisible cues for robot navigation in complex natural environments</title>
      <link>https://evgkanias.github.io/project/invisible-cues/</link>
      <pubDate>Wed, 01 Mar 2017 00:00:00 +0000</pubDate>
      <guid>https://evgkanias.github.io/project/invisible-cues/</guid>
      <description>&lt;p&gt;Outdoor navigation in natural environments remains a challenge for robotics.
Recent breakthroughs in robot navigation have been dependent on specific sensor
technologies, such as laser depth sensors and GPS, and advanced image processing.
The ability of animals such as ants to navigate effectively without such
power- and computation- hungry systems are a proof of principle that
alternative cheaper approaches are viable. Ants also have specialised sensing,
with a peripheral visual system that has evolved to be sensitive to crucial
cues for navigation. Specifically, they make use of non-visible (to humans)
light cues in the form of ultraviolet (UV) and polarised light detection.
UV detection allows the important signal of the horizon shape against the sky
to be easily distinguished. Polarised light detection provides an external
compass cue of heading relative to the sun direction, even when only a small
portion of the sky is visible.&lt;/p&gt;
&lt;p&gt;We propose to build a sensory system that gathers the full range of light
cues available to the ant, in its natural ecological situation, and to analyse
the information contained in this signal. We will also analyse how the
specific sensor layout (ommatidia array), peripheral receptor characteristics,
and the motor behaviour of the ant may contribute to extracting salient
information. The data will form a test-bed for comparison of algorithmic and
neural models of the processing that underlies the navigation capabilities
of the ant. To date, these cues have been considered separately but we believe
the navigational success of this system depends on the specific combination.
For example, the directional information in the polarised sky may form an
important part of visual memories; and UV information may contribute to
disambiguation of the polarisation pattern and the robustness of this
information under different cloud conditions.&lt;/p&gt;
&lt;p&gt;There has been a substantial increase in the last few years in research into
insect neural pathways involved in processing these cues which has yet to be
exploited in robot models. In particular there has been breakthrough work on
the central brain mechanisms involved in decoding polarised light to obtain
heading direction. There is also a rapidly increasing understanding of the
circuits involved in learning, a key component of navigation capabilities.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Miniature Insect Model for Active Learning (Minimal)</title>
      <link>https://evgkanias.github.io/project/minimal/</link>
      <pubDate>Thu, 01 Sep 2016 00:00:00 +0000</pubDate>
      <guid>https://evgkanias.github.io/project/minimal/</guid>
      <description>&lt;p&gt;Biology provides the inspiration for a vision of small low-power devices that are able to learn rapidly and autonomously about environmental contingencies, enabling prediction and adaptive anticipatory action. Larval Drosophila have fewer than 10,000 neurons, yet express a variety of complex orientation and learning behaviours, including non-trivial anticipatory actions requiring context-dependent evaluation of the value of learned cues. Current computational learning theory cannot fully account for or replicate these capacities. We aim to develop a new foundation for understanding natural learning by developing a complete multilevel model of learning in larvae.&lt;/p&gt;
&lt;p&gt;Our aims are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;to analyse at a fine scale how larval olfactory behaviour is controlled and altered by associative conditioning, linked to agent-based models that ground learning capabilities in ongoing sensorimotor control;&lt;/li&gt;
&lt;li&gt;to build one-to-one computational neural models that can be validated by exploiting the recent expansion of the Drosophila neurogenetic toolkit to gain unprecedented ability to characterise and manipulate neural circuits during unconstrained behaviour;&lt;/li&gt;
&lt;li&gt;to derive from these models novel, generalisable algorithms and circuit architectures that can be used to enhance the learning and anticipatory capabilities of machines.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Robocrab: data-driven adaptation of the evation behaviour in fiddler crabs</title>
      <link>https://evgkanias.github.io/project/msc-thesis/</link>
      <pubDate>Mon, 01 Feb 2016 00:00:00 +0000</pubDate>
      <guid>https://evgkanias.github.io/project/msc-thesis/</guid>
      <description>&lt;p&gt;Analysing the behaviour of animals and studying their brain structure is a common
way to create bio-inspired artificial intelligence methods. Fiddler crabs are animals
with specific limitations on their sensors that have a great ability of evading on the
presence of potential predators. In this project we study these animals with respect to
the physiology of their unique sensor – their vision – and their evading manoeuvres,
in order to create a machine learning model, which imitates this interesting behaviour.
More specifically, we propose a semi-supervised architecture of neural network, inspired
by the structure of fiddler crabs’ brain and their evasion behaviour’s feedback
loops. We combine location specific visual units and LSTM recurrent units in order to
build a model capable to adapt this behaviour. We trained our model using a data-set
we created using data from experiments done with living crabs in their habitat. Comparing
our statistical results of our model’s behaviour with the ones of the original
crabs behaviour we show that this model captured some key features of this behaviour
as well as it seems to behave quite realistic in the simulations. Therefore, we show that
it is reasonable to consider machine learning models as potential solutions in animal
behaviour adaptation problems.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
